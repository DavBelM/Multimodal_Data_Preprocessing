{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bd2ccff",
   "metadata": {},
   "source": [
    "# Task 1: Complete Data Merge and Exploratory Data Analysis\n",
    "\n",
    "**Team Members:** Mitali, Blessing, Liliane, Mwai\n",
    "\n",
    "**Objective:**\n",
    "- Load customer_social_profiles and customer_transactions datasets\n",
    "- Perform comprehensive EDA with required visualizations\n",
    "- Clean and preprocess data\n",
    "- Merge datasets with justified logic\n",
    "- Engineer features for product prediction\n",
    "- Save merged dataset for modeling\n",
    "\n",
    "**Note:** This notebook builds upon initial merge work and completes all rubric requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76901e4",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc23d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "\n",
    "# Machine learning preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9897571",
   "metadata": {},
   "source": [
    "## 2. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce140a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "social_df = pd.read_csv('../data/raw/customer_social_profiles.csv')\n",
    "transactions_df = pd.read_csv('../data/raw/customer_transactions.csv')\n",
    "\n",
    "print(f\"Social Profiles Shape: {social_df.shape}\")\n",
    "print(f\"Transactions Shape: {transactions_df.shape}\")\n",
    "print(\"\\nDatasets loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5e05ca",
   "metadata": {},
   "source": [
    "## 3. Initial Data Exploration\n",
    "\n",
    "### 3.1 Social Profiles Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cf2446",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SOCIAL PROFILES DATASET - INITIAL EXPLORATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(social_df.head())\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "social_df.info()\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "display(social_df.describe())\n",
    "\n",
    "print(\"\\nColumn Names:\")\n",
    "print(social_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eeb186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values in Social Profiles:\")\n",
    "missing_social = social_df.isnull().sum()\n",
    "missing_social_pct = (missing_social / len(social_df)) * 100\n",
    "missing_info_social = pd.DataFrame({\n",
    "    'Missing Count': missing_social,\n",
    "    'Percentage': missing_social_pct\n",
    "})\n",
    "display(missing_info_social[missing_info_social['Missing Count'] > 0])\n",
    "\n",
    "if missing_social.sum() == 0:\n",
    "    print(\"No missing values found!\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates_social = social_df.duplicated().sum()\n",
    "print(f\"\\nDuplicate Rows: {duplicates_social}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ead163",
   "metadata": {},
   "source": [
    "### 3.2 Transactions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedfdbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TRANSACTIONS DATASET - INITIAL EXPLORATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(transactions_df.head())\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "transactions_df.info()\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "display(transactions_df.describe())\n",
    "\n",
    "print(\"\\nColumn Names:\")\n",
    "print(transactions_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d13e24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values in Transactions:\")\n",
    "missing_trans = transactions_df.isnull().sum()\n",
    "missing_trans_pct = (missing_trans / len(transactions_df)) * 100\n",
    "missing_info_trans = pd.DataFrame({\n",
    "    'Missing Count': missing_trans,\n",
    "    'Percentage': missing_trans_pct\n",
    "})\n",
    "display(missing_info_trans[missing_info_trans['Missing Count'] > 0])\n",
    "\n",
    "if missing_trans.sum() == 0:\n",
    "    print(\"No missing values found!\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates_trans = transactions_df.duplicated().sum()\n",
    "print(f\"\\nDuplicate Rows: {duplicates_trans}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4217af8c",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning\n",
    "\n",
    "### 4.1 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00791ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copies for cleaning\n",
    "social_clean = social_df.copy()\n",
    "transactions_clean = transactions_df.copy()\n",
    "\n",
    "# Handle missing values if any exist\n",
    "# Strategy: Fill numeric with median, categorical with mode, or drop rows with critical missing data\n",
    "\n",
    "if social_clean.isnull().sum().sum() > 0:\n",
    "    print(\"Handling missing values in social profiles...\")\n",
    "    # Add handling logic based on columns\n",
    "    for col in social_clean.columns:\n",
    "        if social_clean[col].isnull().sum() > 0:\n",
    "            if social_clean[col].dtype in ['int64', 'float64']:\n",
    "                social_clean[col].fillna(social_clean[col].median(), inplace=True)\n",
    "                print(f\"  - Filled {col} with median\")\n",
    "            else:\n",
    "                social_clean[col].fillna(social_clean[col].mode()[0], inplace=True)\n",
    "                print(f\"  - Filled {col} with mode\")\n",
    "else:\n",
    "    print(\"No missing values to handle in social profiles\")\n",
    "\n",
    "if transactions_clean.isnull().sum().sum() > 0:\n",
    "    print(\"\\nHandling missing values in transactions...\")\n",
    "    for col in transactions_clean.columns:\n",
    "        if transactions_clean[col].isnull().sum() > 0:\n",
    "            if transactions_clean[col].dtype in ['int64', 'float64']:\n",
    "                transactions_clean[col].fillna(transactions_clean[col].median(), inplace=True)\n",
    "                print(f\"  - Filled {col} with median\")\n",
    "            else:\n",
    "                transactions_clean[col].fillna(transactions_clean[col].mode()[0], inplace=True)\n",
    "                print(f\"  - Filled {col} with mode\")\n",
    "else:\n",
    "    print(\"No missing values to handle in transactions\")\n",
    "\n",
    "print(\"\\nMissing value handling complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79154642",
   "metadata": {},
   "source": [
    "### 4.2 Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb7b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates\n",
    "rows_before_social = len(social_clean)\n",
    "social_clean = social_clean.drop_duplicates()\n",
    "rows_removed_social = rows_before_social - len(social_clean)\n",
    "\n",
    "rows_before_trans = len(transactions_clean)\n",
    "transactions_clean = transactions_clean.drop_duplicates()\n",
    "rows_removed_trans = rows_before_trans - len(transactions_clean)\n",
    "\n",
    "print(f\"Social Profiles - Rows removed: {rows_removed_social}\")\n",
    "print(f\"Transactions - Rows removed: {rows_removed_trans}\")\n",
    "print(f\"\\nFinal shapes:\")\n",
    "print(f\"  Social: {social_clean.shape}\")\n",
    "print(f\"  Transactions: {transactions_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741a9d7",
   "metadata": {},
   "source": [
    "### 4.3 Standardize Column Names and Fix Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcbae85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize customer ID columns for merging\n",
    "print(\"Standardizing column names...\")\n",
    "\n",
    "# Rename columns to have consistent customer_id\n",
    "if 'customer_id_new' in social_clean.columns:\n",
    "    social_clean.rename(columns={'customer_id_new': 'customer_id'}, inplace=True)\n",
    "    print(\"  - Renamed 'customer_id_new' to 'customer_id' in social profiles\")\n",
    "\n",
    "if 'customer_id_legacy' in transactions_clean.columns:\n",
    "    transactions_clean.rename(columns={'customer_id_legacy': 'customer_id'}, inplace=True)\n",
    "    print(\"  - Renamed 'customer_id_legacy' to 'customer_id' in transactions\")\n",
    "\n",
    "# Extract numeric ID from social profiles if needed (e.g., \"CUST-150\" -> 150)\n",
    "if social_clean['customer_id'].dtype == 'object':\n",
    "    print(\"\\nExtracting numeric customer_id from social profiles...\")\n",
    "    social_clean['customer_id'] = social_clean['customer_id'].str.extract('(\\d+)').astype(int)\n",
    "    print(\"  - Conversion complete\")\n",
    "\n",
    "print(\"\\nColumn standardization complete!\")\n",
    "print(f\"Social customer_id dtype: {social_clean['customer_id'].dtype}\")\n",
    "print(f\"Transactions customer_id dtype: {transactions_clean['customer_id'].dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b51f1",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### Required: At least 3 labeled visualizations showing distributions, outliers, and correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0423f21",
   "metadata": {},
   "source": [
    "### 5.1 Plot 1: Distribution of Numerical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2c82e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical columns from both datasets\n",
    "social_numeric = social_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "trans_numeric = transactions_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove customer_id from visualization\n",
    "social_numeric = [col for col in social_numeric if col != 'customer_id']\n",
    "trans_numeric = [col for col in trans_numeric if col != 'customer_id']\n",
    "\n",
    "print(\"Numerical columns in social profiles:\", social_numeric)\n",
    "print(\"Numerical columns in transactions:\", trans_numeric)\n",
    "\n",
    "# Plot distributions for social profiles\n",
    "if len(social_numeric) > 0:\n",
    "    n_cols = min(3, len(social_numeric))\n",
    "    n_rows = (len(social_numeric) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    axes = axes.flatten() if len(social_numeric) > 1 else [axes]\n",
    "    \n",
    "    for idx, col in enumerate(social_numeric):\n",
    "        social_clean[col].hist(bins=30, ax=axes[idx], edgecolor='black', alpha=0.7)\n",
    "        axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel(col, fontsize=10)\n",
    "        axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(social_numeric), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Social Profiles - Distribution of Numerical Features', \n",
    "                 fontsize=14, fontweight='bold', y=1.002)\n",
    "    plt.show()\n",
    "\n",
    "# Plot distributions for transactions\n",
    "if len(trans_numeric) > 0:\n",
    "    n_cols = min(3, len(trans_numeric))\n",
    "    n_rows = (len(trans_numeric) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    axes = axes.flatten() if len(trans_numeric) > 1 else [axes]\n",
    "    \n",
    "    for idx, col in enumerate(trans_numeric):\n",
    "        transactions_clean[col].hist(bins=30, ax=axes[idx], edgecolor='black', alpha=0.7, color='green')\n",
    "        axes[idx].set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_xlabel(col, fontsize=10)\n",
    "        axes[idx].set_ylabel('Frequency', fontsize=10)\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(len(trans_numeric), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Transactions - Distribution of Numerical Features', \n",
    "                 fontsize=14, fontweight='bold', y=1.002)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- The histograms show the distribution patterns of numerical features\")\n",
    "print(\"- Look for skewness, multimodality, or unusual patterns\")\n",
    "print(\"- Normal distributions appear bell-shaped, while skewed distributions lean left or right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f130a27",
   "metadata": {},
   "source": [
    "### 5.2 Plot 2: Box Plots for Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f57b36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box plots for outlier detection - Social Profiles\n",
    "if len(social_numeric) > 0:\n",
    "    fig, axes = plt.subplots(1, len(social_numeric), figsize=(5*len(social_numeric), 6))\n",
    "    if len(social_numeric) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, col in enumerate(social_numeric):\n",
    "        axes[idx].boxplot(social_clean[col].dropna(), vert=True)\n",
    "        axes[idx].set_title(f'Box Plot: {col}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_ylabel(col, fontsize=10)\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Social Profiles - Outlier Detection (Box Plots)', \n",
    "                 fontsize=14, fontweight='bold', y=1.002)\n",
    "    plt.show()\n",
    "\n",
    "# Box plots for transactions\n",
    "if len(trans_numeric) > 0:\n",
    "    fig, axes = plt.subplots(1, len(trans_numeric), figsize=(5*len(trans_numeric), 6))\n",
    "    if len(trans_numeric) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, col in enumerate(trans_numeric):\n",
    "        axes[idx].boxplot(transactions_clean[col].dropna(), vert=True)\n",
    "        axes[idx].set_title(f'Box Plot: {col}', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_ylabel(col, fontsize=10)\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('Transactions - Outlier Detection (Box Plots)', \n",
    "                 fontsize=14, fontweight='bold', y=1.002)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Box plots show the median (middle line), quartiles (box boundaries), and outliers (dots)\")\n",
    "print(\"- Outliers are data points that fall significantly outside the typical range\")\n",
    "print(\"- Long whiskers or many outlier points may indicate extreme values that need investigation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf47752",
   "metadata": {},
   "source": [
    "### 5.3 Plot 3: Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2812a6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap for social profiles\n",
    "if len(social_numeric) > 1:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_social = social_clean[social_numeric].corr()\n",
    "    sns.heatmap(correlation_social, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, linewidths=1, fmt='.2f', cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Correlation Heatmap - Social Profiles', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nStrong correlations in social profiles:\")\n",
    "    # Find strong correlations (abs value > 0.5, excluding diagonal)\n",
    "    strong_corr = []\n",
    "    for i in range(len(correlation_social.columns)):\n",
    "        for j in range(i+1, len(correlation_social.columns)):\n",
    "            if abs(correlation_social.iloc[i, j]) > 0.5:\n",
    "                strong_corr.append((correlation_social.columns[i], \n",
    "                                   correlation_social.columns[j], \n",
    "                                   correlation_social.iloc[i, j]))\n",
    "    if strong_corr:\n",
    "        for col1, col2, corr_val in strong_corr:\n",
    "            print(f\"  - {col1} vs {col2}: {corr_val:.3f}\")\n",
    "    else:\n",
    "        print(\"  - No strong correlations found (threshold: |r| > 0.5)\")\n",
    "\n",
    "# Correlation heatmap for transactions\n",
    "if len(trans_numeric) > 1:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_trans = transactions_clean[trans_numeric].corr()\n",
    "    sns.heatmap(correlation_trans, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, linewidths=1, fmt='.2f', cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Correlation Heatmap - Transactions', fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nStrong correlations in transactions:\")\n",
    "    strong_corr = []\n",
    "    for i in range(len(correlation_trans.columns)):\n",
    "        for j in range(i+1, len(correlation_trans.columns)):\n",
    "            if abs(correlation_trans.iloc[i, j]) > 0.5:\n",
    "                strong_corr.append((correlation_trans.columns[i], \n",
    "                                   correlation_trans.columns[j], \n",
    "                                   correlation_trans.iloc[i, j]))\n",
    "    if strong_corr:\n",
    "        for col1, col2, corr_val in strong_corr:\n",
    "            print(f\"  - {col1} vs {col2}: {corr_val:.3f}\")\n",
    "    else:\n",
    "        print(\"  - No strong correlations found (threshold: |r| > 0.5)\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Values close to +1 indicate strong positive correlation\")\n",
    "print(\"- Values close to -1 indicate strong negative correlation\")\n",
    "print(\"- Values close to 0 indicate weak or no linear relationship\")\n",
    "print(\"- Strong correlations may indicate redundant features or important relationships\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c450e2ad",
   "metadata": {},
   "source": [
    "### 5.4 Additional EDA: Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5212e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical variables\n",
    "social_categorical = social_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "trans_categorical = transactions_clean.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(\"Categorical columns in social profiles:\", social_categorical)\n",
    "print(\"Categorical columns in transactions:\", trans_categorical)\n",
    "\n",
    "# Value counts for categorical variables\n",
    "if len(social_categorical) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SOCIAL PROFILES - CATEGORICAL VARIABLE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    for col in social_categorical:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(social_clean[col].value_counts())\n",
    "        print(f\"Unique values: {social_clean[col].nunique()}\")\n",
    "\n",
    "if len(trans_categorical) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRANSACTIONS - CATEGORICAL VARIABLE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    for col in trans_categorical:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(transactions_clean[col].value_counts())\n",
    "        print(f\"Unique values: {transactions_clean[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221d300d",
   "metadata": {},
   "source": [
    "## 6. Data Merge\n",
    "\n",
    "### 6.1 Merge Strategy and Justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c00881",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MERGE STRATEGY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nJoin Key: customer_id\")\n",
    "print(\"\\nJustification:\")\n",
    "print(\"- Both datasets contain customer_id as the linking field\")\n",
    "print(\"- This allows us to combine social profile information with transaction history\")\n",
    "print(\"- Enables analysis of how social media behavior relates to purchasing patterns\")\n",
    "\n",
    "print(\"\\nMerge Type: INNER JOIN\")\n",
    "print(\"\\nReason:\")\n",
    "print(\"- We only want customers who have both social profiles AND transaction history\")\n",
    "print(\"- This ensures complete data for product recommendation modeling\")\n",
    "print(\"- Customers without either component cannot be properly analyzed\")\n",
    "\n",
    "print(\"\\nPre-merge Analysis:\")\n",
    "print(f\"- Unique customers in social profiles: {social_clean['customer_id'].nunique()}\")\n",
    "print(f\"- Unique customers in transactions: {transactions_clean['customer_id'].nunique()}\")\n",
    "print(f\"- Customers in both datasets: {len(set(social_clean['customer_id']) & set(transactions_clean['customer_id']))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c135264",
   "metadata": {},
   "source": [
    "### 6.2 Aggregate Transaction Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67dd0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since one customer may have multiple transactions, aggregate them\n",
    "print(\"Aggregating transaction data per customer...\")\n",
    "\n",
    "# Check how many transactions per customer\n",
    "trans_per_customer = transactions_clean.groupby('customer_id').size()\n",
    "print(f\"\\nAverage transactions per customer: {trans_per_customer.mean():.2f}\")\n",
    "print(f\"Max transactions per customer: {trans_per_customer.max()}\")\n",
    "print(f\"Min transactions per customer: {trans_per_customer.min()}\")\n",
    "\n",
    "# Aggregate transactions\n",
    "agg_functions = {}\n",
    "for col in trans_numeric:\n",
    "    if col in transactions_clean.columns:\n",
    "        agg_functions[col] = ['mean', 'sum', 'std']\n",
    "\n",
    "# Add categorical aggregations\n",
    "for col in trans_categorical:\n",
    "    if col in transactions_clean.columns and col != 'customer_id':\n",
    "        agg_functions[col] = lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0]\n",
    "\n",
    "transactions_agg = transactions_clean.groupby('customer_id').agg(agg_functions).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "transactions_agg.columns = ['_'.join(col).strip('_') if col[1] else col[0] \n",
    "                            for col in transactions_agg.columns.values]\n",
    "\n",
    "print(f\"\\nAggregated transactions shape: {transactions_agg.shape}\")\n",
    "print(\"\\nAggregated columns:\")\n",
    "print(transactions_agg.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e6f61",
   "metadata": {},
   "source": [
    "### 6.3 Perform Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22167622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform inner join\n",
    "merged_df = pd.merge(\n",
    "    social_clean,\n",
    "    transactions_agg,\n",
    "    on='customer_id',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"Merged dataset shape: {merged_df.shape}\")\n",
    "print(f\"\\nRows in social profiles: {len(social_clean)}\")\n",
    "print(f\"Rows in transactions (aggregated): {len(transactions_agg)}\")\n",
    "print(f\"Rows after merge: {len(merged_df)}\")\n",
    "print(f\"\\nData loss: {len(social_clean) - len(merged_df)} customers from social profiles\")\n",
    "\n",
    "print(\"\\nFirst 5 rows of merged dataset:\")\n",
    "display(merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bf8e21",
   "metadata": {},
   "source": [
    "### 6.4 Validate Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d51a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MERGE VALIDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check for unexpected nulls\n",
    "print(\"\\nMissing values after merge:\")\n",
    "missing_merged = merged_df.isnull().sum()\n",
    "if missing_merged.sum() > 0:\n",
    "    display(missing_merged[missing_merged > 0])\n",
    "else:\n",
    "    print(\"No missing values!\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates_merged = merged_df.duplicated().sum()\n",
    "print(f\"\\nDuplicate rows: {duplicates_merged}\")\n",
    "\n",
    "# Verify unique customers\n",
    "unique_customers = merged_df['customer_id'].nunique()\n",
    "print(f\"\\nUnique customers in merged dataset: {unique_customers}\")\n",
    "print(f\"Total rows: {len(merged_df)}\")\n",
    "\n",
    "if unique_customers == len(merged_df):\n",
    "    print(\"Validation passed: One row per customer\")\n",
    "else:\n",
    "    print(\"Warning: Multiple rows per customer detected\")\n",
    "\n",
    "print(\"\\nMerged dataset info:\")\n",
    "merged_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f237723e",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering\n",
    "\n",
    "Create new features to improve product recommendation model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4533a1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating engineered features...\\n\")\n",
    "\n",
    "# Create a copy for feature engineering\n",
    "merged_engineered = merged_df.copy()\n",
    "\n",
    "# Feature 1: Customer value score (if purchase amount exists)\n",
    "if any('purchase_amount' in col.lower() for col in merged_engineered.columns):\n",
    "    purchase_cols = [col for col in merged_engineered.columns if 'purchase_amount' in col.lower()]\n",
    "    if purchase_cols:\n",
    "        sum_col = [col for col in purchase_cols if 'sum' in col.lower()]\n",
    "        if sum_col:\n",
    "            merged_engineered['customer_value_score'] = merged_engineered[sum_col[0]]\n",
    "            print(\"Created: customer_value_score (total purchase amount)\")\n",
    "\n",
    "# Feature 2: Average transaction value\n",
    "if any('purchase_amount_mean' in col.lower() for col in merged_engineered.columns):\n",
    "    mean_cols = [col for col in merged_engineered.columns if 'purchase_amount_mean' in col.lower()]\n",
    "    if mean_cols:\n",
    "        merged_engineered['avg_transaction_value'] = merged_engineered[mean_cols[0]]\n",
    "        print(\"Created: avg_transaction_value\")\n",
    "\n",
    "# Feature 3: Transaction consistency (std deviation normalized)\n",
    "if any('purchase_amount_std' in col.lower() for col in merged_engineered.columns) and \\\n",
    "   any('purchase_amount_mean' in col.lower() for col in merged_engineered.columns):\n",
    "    std_cols = [col for col in merged_engineered.columns if 'purchase_amount_std' in col.lower()]\n",
    "    mean_cols = [col for col in merged_engineered.columns if 'purchase_amount_mean' in col.lower()]\n",
    "    if std_cols and mean_cols:\n",
    "        merged_engineered['transaction_consistency'] = (\n",
    "            merged_engineered[std_cols[0]] / (merged_engineered[mean_cols[0]] + 1)\n",
    "        )\n",
    "        print(\"Created: transaction_consistency (coefficient of variation)\")\n",
    "\n",
    "# Feature 4: Social engagement score (if relevant columns exist)\n",
    "social_engagement_cols = [col for col in merged_engineered.columns \n",
    "                          if any(keyword in col.lower() for keyword in \n",
    "                                ['follower', 'like', 'post', 'engagement'])]\n",
    "if social_engagement_cols:\n",
    "    # Normalize and sum engagement metrics\n",
    "    scaler = StandardScaler()\n",
    "    engagement_scaled = scaler.fit_transform(merged_engineered[social_engagement_cols].fillna(0))\n",
    "    merged_engineered['social_engagement_score'] = engagement_scaled.mean(axis=1)\n",
    "    print(f\"Created: social_engagement_score (from {len(social_engagement_cols)} metrics)\")\n",
    "\n",
    "# Feature 5: Customer rating score (if exists)\n",
    "rating_cols = [col for col in merged_engineered.columns if 'rating' in col.lower()]\n",
    "if rating_cols:\n",
    "    merged_engineered['avg_customer_rating'] = merged_engineered[rating_cols[0]]\n",
    "    print(\"Created: avg_customer_rating\")\n",
    "\n",
    "print(f\"\\nTotal features after engineering: {merged_engineered.shape[1]}\")\n",
    "print(\"\\nNew feature columns:\")\n",
    "new_features = [col for col in merged_engineered.columns if col not in merged_df.columns]\n",
    "print(new_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e1b307",
   "metadata": {},
   "source": [
    "## 8. Define Target Variable\n",
    "\n",
    "Identify or create the target variable for product recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69e6ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for product-related columns\n",
    "product_cols = [col for col in merged_engineered.columns if 'product' in col.lower()]\n",
    "print(\"Product-related columns found:\")\n",
    "print(product_cols)\n",
    "\n",
    "# If product_category exists, use it as target\n",
    "if product_cols:\n",
    "    target_col = product_cols[0]\n",
    "    print(f\"\\nTarget variable: {target_col}\")\n",
    "    print(f\"\\nTarget distribution:\")\n",
    "    print(merged_engineered[target_col].value_counts())\n",
    "    \n",
    "    # Visualize target distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    merged_engineered[target_col].value_counts().plot(kind='bar', edgecolor='black')\n",
    "    plt.title(f'Distribution of Target Variable: {target_col}', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(target_col, fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nNo obvious product column found. Manual target creation may be needed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f556d45",
   "metadata": {},
   "source": [
    "## 9. Final Dataset Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0759ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINAL DATASET SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nShape: {merged_engineered.shape}\")\n",
    "print(f\"Number of customers: {merged_engineered['customer_id'].nunique()}\")\n",
    "print(f\"Number of features: {merged_engineered.shape[1]}\")\n",
    "\n",
    "print(\"\\nColumn types:\")\n",
    "print(merged_engineered.dtypes.value_counts())\n",
    "\n",
    "print(\"\\nMemory usage:\")\n",
    "print(f\"{merged_engineered.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "print(\"\\nSample of final dataset:\")\n",
    "display(merged_engineered.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2883488f",
   "metadata": {},
   "source": [
    "## 10. Save Processed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ace3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to processed folder\n",
    "output_path = '../data/processed/merged_customer_data.csv'\n",
    "merged_engineered.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Dataset saved successfully to: {output_path}\")\n",
    "print(f\"\\nFile size: {merged_engineered.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Rows: {len(merged_engineered)}\")\n",
    "print(f\"Columns: {len(merged_engineered.columns)}\")\n",
    "\n",
    "# Verify save\n",
    "test_load = pd.read_csv(output_path)\n",
    "print(f\"\\nVerification: Successfully loaded {len(test_load)} rows from saved file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6c4f5f",
   "metadata": {},
   "source": [
    "## 11. Key Findings and Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9638893b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"KEY FINDINGS AND INSIGHTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATA QUALITY:\")\n",
    "print(f\"   - Started with {len(social_df)} social profiles and {len(transactions_df)} transactions\")\n",
    "print(f\"   - After cleaning and merging: {len(merged_engineered)} customers with complete data\")\n",
    "print(f\"   - Data retention rate: {len(merged_engineered)/len(social_df)*100:.1f}%\")\n",
    "\n",
    "print(\"\\n2. MERGE STRATEGY:\")\n",
    "print(\"   - Used inner join on customer_id to ensure complete records\")\n",
    "print(\"   - Aggregated multiple transactions per customer into summary statistics\")\n",
    "print(\"   - Result: One row per customer with comprehensive features\")\n",
    "\n",
    "print(\"\\n3. FEATURE ENGINEERING:\")\n",
    "print(f\"   - Created {len(new_features)} new features from raw data\")\n",
    "print(\"   - Combined social media metrics with transaction patterns\")\n",
    "print(\"   - Features ready for machine learning model training\")\n",
    "\n",
    "print(\"\\n4. NEXT STEPS:\")\n",
    "print(\"   - Use this dataset for product recommendation model (Task 4)\")\n",
    "print(\"   - Apply classification/regression algorithms\")\n",
    "print(\"   - Evaluate model performance with accuracy, F1-score, and loss metrics\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK 1 COMPLETE - Dataset ready for modeling!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
